# HIT_CSmaster_Course
### 模式识别(刘岩)
##### 教材
黑皮书 《模式分类》第二版 Richard O.Duda等，机械工业出版社
##### 复习范围
所有PPT，书上全部内容，有些多、难。复印店可以买到往年题，可以结合题复习，务必掌握。
##### 考题回忆
难度较大，不复习不刷题肯定挂，复习了不刷题多半挂。第一道大题为概念解释，比较基础，比如过拟合的含义和解决方法等，不用担心。后面考了Parzen窗的计算、贝叶斯分类器的计算、反向传播的推导、手动构建神经网络(包括建网和赋权重，以及解释原因。最简单的比如：构建与运算的感知器，当然当时考的比这个难一些，不过总体思想类似)
##### 复习建议
平时课去不去区别不大，嘘_(:з」∠)_ 但是实验一定亲自做，而且要理解。当时Parzen窗我自己写都理解了很久才明白，不复现是肯定理解有偏差的。还有反向传播这种级别算法推导就不需要多说了吧。另外，打印店的资料有些鱼龙混杂，试题没写学校的很多是网上随便搜的，因此不要一股脑看见题就乱刷。
##### 实验
实验一 https://github.com/HuiyanWen/ParzenWindow-PR_Experiment
实验二 https://github.com/HuiyanWen/BP_Network
##### PS
教材编写的内容很全，涵盖了理论证明，可能对初学者不太友好，但如果能硬着头皮啃下来，收获是很大很大的。书中编写有一些错误，不过总体还凑合。


### 机器学习(毕建东/刘扬)
##### 教材
黑皮书 《机器学习》 Mitchell，机械工业出版社
##### 复习范围
PPT大部分内容，第三、九、十、十一、十二章不在考试范围。需要掌握算法的流程、计算、应用、存在的不足和改进的方法，对于梯度下降、模式理论和Q学习的收敛性需要掌握证明。
##### 需掌握的算法
候选消除算法、GS、AQ、ID3、FIND-S、梯度下降算法、遗传算法、KNN、FOIL、Q学习、最小属性子集算法、PCA、Kmeans、GMM、EM
##### 考题回忆
考了候选消除算法的计算、GS算法的计算和默写、Q学习的收敛性证明、Kmeans的流程以及它和GMM的联系、梯度下降算法的流程、遗传算法的流程、概念学习的归纳偏执(对于无偏和有偏的学习器，假设空间分别有多少个假设，如书P29的推导)
##### 复习建议
考的比较简单，但需要复习得很细致，建议至少对着PPT跟着书看一遍。如果是单纯应付考试，EM这种大BOSS可以少花些精力，它的推导和计算明显考不了，最多考算法流程，你可以死记。另外，记算法的时候应多动笔默写，多看它相关的应用实例加深理解，当时考试结束后周围同学不少吐槽算法没完全记住。
##### 实验
机器学习相关算法均可，但需要自己实现或改进，不可使用TensorFlow、Pytorch这样的框架。
##### PS
平时课能去就去，毕老师讲的比较通俗，跟着听课能在复习的时候节省不少力气，后面刘扬老师讲的几节课浓缩得很厉害，比较难，不要被吓到。顺便，不得不吐槽教材教材，编写不是很好，比如反向传播的推导，Mitchell将不同层的权重都用Wij表示了，完全区分不开，这部分写的还不如上面模式识别的教材，真是全靠同行衬托啊，它的内容也有些过时。对比本科的时候旁听的刘扬老师的机器学习，硕士的机器学习简单了不少，最直白的就是实验内容，当时他们本科有4个实验，都是常用而且较难的算法。这也就意味着，如果想要从事ML开发和研究，除了这门课，还需要自己补很多内容。


### 最优化方法/运筹学
##### 教材
《运筹学与最优化方法》第二版，吴祈宗&侯福均，机械工业出版社
##### 复习范围
<br>老师考前划的重点见项目文件，此外还划了几道题：
<br>第一章 P21：8
<br>第二章 P38：2、3、7(答案分别是严格凸、严格凹、严格凸、x1≥-23/36、非凸非凹)、10(均为凸规划)
<br>第三章 P83：1、5(无最优解、4、12、68/3)、7、12(22/3、忘记了)、15(225、100、不影响、965/8、没有、95)
<br>第四章 P109：6、7
<br>第五章 P131：1、2
<br>第六章 P171：2、3、10(仅用既约梯度法)、11、12
##### 考题回忆
今年的题比往年难了不少，第一、二大题各两小问，对凸集和凸函数判定，要求只用定义证明，挺难的。特别是老师提前说了证明不怎么考，就没有花力气攻克这块。后面分别有牛顿法、单纯形法和灵敏度分析、闸函数法、既约梯度法计算，计算量和难度都不低。
##### 复习建议
时间有限的情况下，可以以做题为驱动复习。上述题基本涵盖了哈工大硕士最优化课程的内容，但不包含凸集、凸函数、凸规划的证明，共轭梯度法，凸单纯形法、KT条件的计算等内容。两个容易错的地方：一是应注重算法的标准形式，比如单纯形法需提前变换为目标函数为max形式，约束条件为等式，且右端项b全大于等于0的标准形式，而对偶单纯形法需要提前化成max、≤的形式，还有凸规划的标准形式、库恩塔克条件的标准形式以及闸函数法和罚函数法的标准形式都需要区分开。二是KT条件的一般形式，即在约束条件有等式时，KT表达式怎么写。
##### PS
总的来说，就是多做题、多动笔，有条件的情况下，最好明白算法的原理，比如单纯形表格法的由来，它和对偶单纯形法、大M法、两阶段法的适用条件。对于考试来说，最低的要求是会正确运用。


### 自然语言处理(关毅)
往届的考点可以参考https://www.cnblogs.com/szxspark/p/10262161.html ，到我们这届关老师一时兴起取消了考试，改为100%靠论文打分，关于如何跨越语义鸿沟，写一篇综述类的报告并提出自己的看法。当然这里不是指图像里或其他领域的语义鸿沟，而是指句法分析的瓶颈所导致的语义鸿沟。个人觉得，大论文并不比综述简单，100%成绩来自于论文意味着你的论文在格式和内容上都要大大优于你的同学，才有可能得到高分。我当时花了大概一周的之间从早写到晚，才写了一篇小综述，而且感觉写得不太好。所以，能让老师考试还是建议考试，至少基础还能扎实一点。


### 中国特色社会主义
没什么好说的，课一定要去，我们这一届被抓到旷课的同学，最后打分基本60起步，顶天70。PPT在打印店都有整理好的，复习的时候浏览一遍，能快速找到即可。考试时尽量多写。
