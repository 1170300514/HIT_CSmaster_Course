# HIT_CSmaster_Course

### 模式识别(刘岩)




### 机器学习(毕建东/刘扬)
##### 复习范围
PPT大部分内容，第三、九、十、十一、十二章不在考试范围。需要掌握算法的流程、计算、应用、存在的不足和改进的方法，对于梯度下降、模式理论和Q学习的收敛性需要掌握证明。
##### 需掌握的算法
候选消除算法、GS、AQ、ID3、FIND-S、梯度下降算法、遗传算法、KNN、FOIL、Q学习、最小属性子集算法、PCA、Kmeans、GMM、EM
##### 考题回忆
考了候选消除算法的计算、GS算法的计算和默写、Q学习的收敛性证明、Kmeans的流程以及它和GMM的联系、梯度下降算法的流程、遗传算法的流程、概念学习的归纳偏执(对于无偏和有偏的学习器，假设空间分别有多少个假设，如书P29的推导)
##### 复习建议
考的比较简单，但需要复习得很细致，建议至少对着PPT跟着书看一遍。另外，记算法的时候应多动笔默写，多看它相关的应用实例加深理解，当时考试结束后周围同学不少吐槽算法没完全记住。该课程的教材——Mitchell的《机器学习》编写得并不好，而且内容有些过时。对比本科的时候旁听的刘扬老师的机器学习，硕士的机器学习简单了不少，也就意味着如果想要从事AI开发和研究，还需要自己补很多内容。
##### 实验



### 最优化方法/运筹学
##### 复习范围
<br>课程所用的课本是《运筹学与最优化方法》第二版，吴祈宗&侯福均。老师考前划的重点见项目文件，此外还划了几道题：
<br>第一章 P21：8
<br>第二章 P38：2、3、7(答案分别是严格凸、严格凹、严格凸、x1≥-23/36、非凸非凹)、10(均为凸规划)
<br>第三章 P83：1、5(无最优解、4、12、68/3)、7、12(22/3、忘记了)、15(225、100、不影响、965/8、没有、95)
<br>第四章 P109：6、7
<br>第五章 P131：1、2
<br>第六章 P171：2、3、10(仅用既约梯度法)、11、12
##### 考题回忆
今年的题比往年难了不少，第一、二大题各两小问，对凸集和凸函数判定，要求只用定义证明，挺难的。特别是老师提前说了证明不怎么考，就没有花力气攻克这块。后面分别有牛顿法、单纯形法和灵敏度分析、闸函数法、既约梯度法计算，计算量和难度都不低。
##### 复习建议
时间有限的情况下，可以以做题为驱动复习。上述题基本涵盖了哈工大硕士最优化课程的内容，但不包含凸集、凸函数、凸规划的证明，共轭梯度法，凸单纯形法、KT条件的计算等内容。两个容易错的地方：一是应注重算法的标准形式，比如单纯形法需提前变换为目标函数为max形式，约束条件为等式，且右端项b全大于等于0的标准形式，而对偶单纯形法需要提前化成max、≤的形式，还有凸规划的标准形式、库恩塔克条件的标准形式以及闸函数法和罚函数法的标准形式都需要区分开。二是KT条件的一般形式，即在约束条件有等式时，KT表达式怎么写。

<br>总的来说，就是多做题、多动笔，有条件的情况下，最好明白算法的原理，比如单纯形表格法的由来，它和对偶单纯形法、大M法、两阶段法的适用条件。对于考试来说，最低的要求是会正确运用。
### 自然语言处理(关毅)
往届的考点可以参考https://www.cnblogs.com/szxspark/p/10262161.html ，到我们这届关老师一时兴起取消了考试，改为100%靠论文打分，关于如何跨越语义鸿沟，写一篇综述类的报告并提出自己的看法。当然这里不是指图像里或其他领域的语义鸿沟，而是指句法分析的瓶颈所导致的语义鸿沟。个人觉得，大论文并不比综述简单，100%成绩来自于论文意味着你的论文在格式和内容上都要大大优于你的同学，才有可能得到高分。我当时花了大概一周的之间从早写到晚，才写了一篇小综述，而且感觉写得不太好。所以，能让老师考试还是建议考试，至少基础还能扎实一点。
